{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPBTwFooEpKoAmKth0xkbGF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Before creating the DNN model we need to import Pandas, Numpy and Tensorflow. We need Pandas for loading and manipulating the dataset, Numpy for numerical operations, TensorFlow for building and training the models, and the MinMaxScaler function from the Sklearn library to normalise the feature and target variables."],"metadata":{"id":"IdKhsBAnAP8c"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.preprocessing import MinMaxScaler"],"metadata":{"id":"xTOCL243ANuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After that we need to load the data that will be used to train the model."],"metadata":{"id":"7o0RANwfAVf7"}},{"cell_type":"code","source":["df = pd.read_csv('https://raw.githubusercontent.com/23009256uhi/23009256_DataAnalytics/main/model_data_dnn.csv')"],"metadata":{"id":"Nou8joEXAXuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.head())\n","print(df.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FBxmINW_XS8","executionInfo":{"status":"ok","timestamp":1702475517701,"user_tz":0,"elapsed":26,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"f5f06921-a7b2-4cbc-be68-348251a8c758"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   day  year  mo  da collision_date  temp  dewp     slp  visib  wdsp  mxpsd  \\\n","0    2  2018   9  23     2018-09-23  59.8  50.2  1023.4   10.0   3.0    5.1   \n","1    6  2018  12  20     2018-12-20  38.6  34.4  1020.2    9.6   5.0    7.0   \n","2    4  2013  11   5     2013-11-05  43.5  30.4  1037.8   10.0   3.9    7.0   \n","3    5  2012   7  11     2012-07-11  77.1  62.0  1019.9   10.0   1.9    7.0   \n","4    6  2012   8   9     2012-08-09  78.2  69.6  1013.6    9.3   2.3    7.0   \n","\n","   gust   max   min  prcp  fog  NUM_COLLISIONS   z_score  \n","0   NaN  78.1  53.1   0.0    0             475 -0.112394  \n","1   NaN  48.0  21.0   0.0    0             806  1.942315  \n","2   NaN  50.0  37.9   0.0    0             510  0.104871  \n","3   NaN  84.0  64.9   0.0    0             565  0.446288  \n","4  15.0  88.0  61.0   0.0    0             581  0.545609  \n","              day         year          mo          da        temp  \\\n","count  662.000000   662.000000  662.000000  662.000000  662.000000   \n","mean     3.895770  2015.580060    7.277946   16.069486   59.225076   \n","std      1.987026     2.240886    2.631951    8.853558   12.978881   \n","min      1.000000  2012.000000    1.000000    1.000000   22.200000   \n","25%      2.000000  2014.000000    6.000000    9.000000   51.000000   \n","50%      4.000000  2016.000000    8.000000   16.500000   63.000000   \n","75%      6.000000  2018.000000    9.000000   24.000000   68.875000   \n","max      7.000000  2019.000000   12.000000   31.000000   85.300000   \n","\n","             dewp          slp      visib        wdsp       mxpsd       gust  \\\n","count  662.000000   662.000000  662.00000  662.000000  662.000000  68.000000   \n","mean    52.460423  1019.041541    8.54864    6.061329   10.609215  17.977941   \n","std     14.408704     6.258055    2.12104    1.753609    1.549903   2.661379   \n","min      8.300000   999.400000    1.50000    0.800000    5.100000  14.000000   \n","25%     43.625000  1015.000000    7.70000    5.000000    9.900000  15.900000   \n","50%     56.700000  1019.100000    9.70000    6.200000   11.100000  17.100000   \n","75%     64.000000  1023.000000   10.00000    7.300000   12.000000  19.000000   \n","max     73.600000  1038.900000   10.00000   10.300000   13.000000  27.000000   \n","\n","              max         min        prcp         fog  NUM_COLLISIONS  \\\n","count  662.000000  662.000000  662.000000  662.000000      662.000000   \n","mean    67.903021   50.584894    0.037538    0.305136      593.027190   \n","std     13.543159   13.583503    0.171152    0.460813       89.757713   \n","min     28.000000   10.000000    0.000000    0.000000      264.000000   \n","25%     60.100000   41.275000    0.000000    0.000000      528.250000   \n","50%     72.000000   54.000000    0.000000    0.000000      596.000000   \n","75%     78.100000   61.000000    0.000000    1.000000      654.750000   \n","max    100.000000   73.000000    2.370000    1.000000      841.000000   \n","\n","          z_score  \n","count  662.000000  \n","mean     0.620269  \n","std      0.557178  \n","min     -1.422194  \n","25%      0.218159  \n","50%      0.638723  \n","75%      1.003418  \n","max      2.159580  \n"]}]},{"cell_type":"markdown","source":["Let's check if there are any NaN values."],"metadata":{"id":"uFruQEU4bqJj"}},{"cell_type":"code","source":["print(df.isna().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-1gz-iFpP_l","executionInfo":{"status":"ok","timestamp":1702475517702,"user_tz":0,"elapsed":15,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"25fc264a-38bd-466b-c2bd-cd69ab6a93f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["day                 0\n","year                0\n","mo                  0\n","da                  0\n","collision_date      0\n","temp                0\n","dewp                0\n","slp                 0\n","visib               0\n","wdsp                0\n","mxpsd               0\n","gust              594\n","max                 0\n","min                 0\n","prcp                0\n","fog                 0\n","NUM_COLLISIONS      0\n","z_score             0\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["As 'gust' has almost every entry as 'NaN', we can just drop it. We can also drop the z score and the collision date for the model."],"metadata":{"id":"NR212rFJbu7B"}},{"cell_type":"code","source":["# Drop non-numeric or irrelevant columns\n","df = df.drop(columns=['collision_date', 'z_score', 'gust'])"],"metadata":{"id":"Ggz6qPL82W4l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we have to split the data into a training set and a test set"],"metadata":{"id":"kJKYNSLpnziO"}},{"cell_type":"code","source":["# Train-test split, training set (80% of the data) and test set (the remaining 20%). The training set is used to train the model, the test set is used to evaluate the model.\n","training_set = df.sample(frac=0.8, random_state=0)\n","test_set = df.drop(training_set.index)\n","\n","training_features = training_set.copy()\n","test_features = test_set.copy()\n","\n","training_labels = training_features.pop('NUM_COLLISIONS')\n","test_labels = test_features.pop('NUM_COLLISIONS')"],"metadata":{"id":"d9JtBGoF66tC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also need to create separate scalers for the feature and target variables. The MinMaxScaler function from sklearn library is used for this purpose."],"metadata":{"id":"J1wWPNNPn4ur"}},{"cell_type":"code","source":["# Create a scaler object for the features\n","feature_scaler = MinMaxScaler()\n","\n","# Create a separate scaler object for the target\n","target_scaler = MinMaxScaler()\n","\n","# Fit the scaler to the training data and transform both training and test data\n","training_features_scaled = feature_scaler.fit_transform(training_features)\n","test_features_scaled = feature_scaler.transform(test_features)\n","\n","# Fit the scaler to the training target data and transform both training and test target data\n","training_labels_scaled = target_scaler.fit_transform(training_labels.values.reshape(-1, 1))\n","test_labels_scaled = target_scaler.transform(test_labels.values.reshape(-1, 1))"],"metadata":{"id":"gPKHh2rDi0fL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normaliser = tf.keras.layers.Normalization(axis=-1)\n","normaliser.adapt(np.array(training_features))"],"metadata":{"id":"SB-YR_f2i46C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can create a function to build the model. After building the model using this function, we will compile the model, train the model and lastly evaluate its performance."],"metadata":{"id":"-wG8CCzWte7h"}},{"cell_type":"code","source":["# Model building function\n","def build_dnn_model():\n","    model = tf.keras.Sequential([\n","        layers.Dense(64, activation='relu', input_shape=[len(training_features.keys())]),\n","        layers.Dense(64, activation='relu'),\n","        layers.Dense(32, activation='relu'),\n","        layers.Dense(1)\n","    ])\n","    return model"],"metadata":{"id":"yywaEUDJkusY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Build the model\n","dnn_model = build_dnn_model()"],"metadata":{"id":"g5uzOJnexWX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Compile the model\n","dnn_model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n","                  loss='mean_absolute_error')"],"metadata":{"id":"8Au_yxGmv80m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. Train the model\n","history = dnn_model.fit(\n","    training_features_scaled,\n","    training_labels_scaled,\n","    epochs=100,\n","    validation_split=0.2,\n","    verbose=0\n",")"],"metadata":{"id":"aFDntqwpwDD2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. Evaluate the model\n","dnn_model_results = dnn_model.evaluate(test_features_scaled, test_labels_scaled, verbose=2)\n","print(dnn_model_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oHL6Ux5LwFTV","executionInfo":{"status":"ok","timestamp":1702475530735,"user_tz":0,"elapsed":264,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"d4305e5e-868e-4a7b-b7ce-2a28b48b3a15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5/5 - 0s - loss: 0.1051 - 31ms/epoch - 6ms/step\n","0.1051439493894577\n"]}]},{"cell_type":"markdown","source":["At this point we can test the model by using the November collision data. Firt we have to load the data."],"metadata":{"id":"_NZlotb739va"}},{"cell_type":"code","source":["df_nov = pd.read_csv('https://raw.githubusercontent.com/23009256uhi/23009256_DataAnalytics/main/november_collated_collision_data.csv')"],"metadata":{"id":"9e8JvcKM3-xg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_nov.head())\n","print(df_nov.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36qtV7lf4GHQ","executionInfo":{"status":"ok","timestamp":1702475531044,"user_tz":0,"elapsed":5,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"41de2214-1bda-457f-c4b5-7f975e04c481"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   day  year  mo  da collision_date  temp  dewp     slp  visib  wdsp  mxpsd  \\\n","0    1  2023  11  13     11/13/2023  39.7  29.6  1026.7   10.0   3.8    8.0   \n","1    7  2023  11   5      11/5/2023  53.0  48.0  1016.7    9.4   5.0   11.1   \n","2    7  2023  11  12     11/12/2023  41.2  29.6  1030.0   10.0   7.1   11.1   \n","3    2  2023  11  21     11/21/2023  38.7  24.2  1033.0   10.0   8.8   12.0   \n","4    1  2023  11   6      11/6/2023  48.4  43.7  1019.6    9.9   7.9   13.0   \n","\n","    gust   max   min  prcp   sndp  fog  NUM_COLLISIONS  \n","0  999.9  48.0  30.0   0.0  999.9    0             300  \n","1  999.9  61.0  43.0   0.0  999.9    0             235  \n","2   14.0  54.0  30.9   0.0  999.9    0             233  \n","3   19.0  46.0  30.9   0.0  999.9    0             352  \n","4   19.0  61.0  39.9   0.0  999.9    0             275  \n","             day    year    mo         da       temp       dewp          slp  \\\n","count  30.000000    30.0  30.0  30.000000  30.000000  30.000000    30.000000   \n","mean    3.966667  2023.0  11.0  15.500000  45.440000  36.536667  1018.913333   \n","std     1.973677     0.0   0.0   8.803408   6.051708   9.468441     7.968374   \n","min     1.000000  2023.0  11.0   1.000000  34.100000  17.700000  1004.200000   \n","25%     2.250000  2023.0  11.0   8.250000  41.625000  29.600000  1014.450000   \n","50%     4.000000  2023.0  11.0  15.500000  45.200000  34.750000  1019.350000   \n","75%     5.750000  2023.0  11.0  22.750000  49.150000  44.675000  1025.800000   \n","max     7.000000  2023.0  11.0  30.000000  56.200000  52.700000  1033.000000   \n","\n","          visib       wdsp      mxpsd        gust        max        min  \\\n","count  30.00000  30.000000  30.000000   30.000000  30.000000  30.000000   \n","mean    9.58000  10.380000  17.133333  254.133333  54.093333  36.443333   \n","std     0.94701   3.672676   6.026455  418.515778   5.512489   6.569191   \n","min     6.20000   3.800000   8.000000   14.000000  43.000000  19.900000   \n","25%     9.55000   7.975000  13.000000   24.350000  50.000000  30.900000   \n","50%    10.00000  10.350000  17.100000   27.500000  54.000000  37.000000   \n","75%    10.00000  12.500000  19.525000   49.350000  59.550000  39.900000   \n","max    10.00000  21.800000  38.100000  999.900000  63.000000  52.000000   \n","\n","            prcp          sndp        fog  NUM_COLLISIONS  \n","count  30.000000  3.000000e+01  30.000000       30.000000  \n","mean    0.029000  9.999000e+02   0.066667      255.566667  \n","std     0.060819  2.312607e-13   0.253708       38.338495  \n","min     0.000000  9.999000e+02   0.000000      186.000000  \n","25%     0.000000  9.999000e+02   0.000000      231.500000  \n","50%     0.000000  9.999000e+02   0.000000      253.500000  \n","75%     0.010000  9.999000e+02   0.000000      275.000000  \n","max     0.260000  9.999000e+02   1.000000      352.000000  \n"]}]},{"cell_type":"markdown","source":["We can extract the actual number of collisions from the data to compare it later with the model predictions."],"metadata":{"id":"1WuvWiAhy7q-"}},{"cell_type":"code","source":["# Extract the actual number of collisions\n","actual_collisions = df_nov['NUM_COLLISIONS']"],"metadata":{"id":"SrhrCNiSrgmy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we have to drop the non relative columns."],"metadata":{"id":"xrIUF2StzEUG"}},{"cell_type":"code","source":["df_nov = df_nov.drop(columns=['collision_date', 'sndp', 'gust', 'NUM_COLLISIONS'])"],"metadata":{"id":"68BxDs7E4ov7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next steps involve scaling the data using the same feature scaler used before, making predictions with the model by using the model created before and lastly inverse scaling the predictions to convert them back to their original scale."],"metadata":{"id":"y8vrG3qQ5B3m"}},{"cell_type":"code","source":["# Scale the data\n","nov_features_scaled = feature_scaler.transform(df_nov)\n","\n","# Make predictions with the model\n","nov_predictions_scaled = dnn_model.predict(nov_features_scaled)\n","\n","# Inverse scale the predictions\n","nov_predictions = target_scaler.inverse_transform(nov_predictions_scaled)"],"metadata":{"id":"vvyqDEs14snD","executionInfo":{"status":"ok","timestamp":1702475531445,"user_tz":0,"elapsed":403,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"90209983-5167-4282-bf37-494e0e6fffd9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 95ms/step\n"]}]},{"cell_type":"markdown","source":["Let's now create a new data frame and add the predicted collisions and the actual collisions to check the accuracy of the model."],"metadata":{"id":"7EDqmgKi9KWO"}},{"cell_type":"code","source":["nov_results_df = pd.DataFrame({\n","    'Actual Collisions': actual_collisions,\n","    'Predicted Collisions': nov_predictions.flatten()})\n","\n","print(nov_results_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BvQPwf4qqCt","executionInfo":{"status":"ok","timestamp":1702475531445,"user_tz":0,"elapsed":8,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"32c6ff4a-06a9-4c1a-9ecf-81c5e26fab5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    Actual Collisions  Predicted Collisions\n","0                 300            551.688660\n","1                 235            694.152405\n","2                 233            784.981873\n","3                 352            585.987854\n","4                 275            605.349182\n","5                 240            537.905640\n","6                 320            689.820496\n","7                 290            730.942139\n","8                 203            562.793518\n","9                 263            563.291687\n","10                294            647.820251\n","11                233            684.898621\n","12                242            497.066803\n","13                216            569.610718\n","14                238            715.317810\n","15                266            664.935486\n","16                315            606.781433\n","17                208            561.381714\n","18                270            702.380554\n","19                220            545.986328\n","20                267            714.156250\n","21                275            704.148621\n","22                186            632.040283\n","23                271            629.231140\n","24                215            610.292297\n","25                217            574.759155\n","26                250            726.192505\n","27                285            745.091248\n","28                231            675.496826\n","29                257            645.010010\n"]}]},{"cell_type":"markdown","source":["The DNN model shows a similar pattern to the linear regression model in terms of predicting the number of collisions. In both cases, the models' predictions are approximately double the actual number of collisions. This consistent overstimation might indicate that the models are probably not taking into consideration the changes in traffic patterns due to the pandemic. To improve the models' predictions we can incorporate data from both pre and during COVID. This could help the models adapt to the shifts in traffic patterns caused by the pandemic, and lead to more accurate predictions.  "],"metadata":{"id":"rIQdzrQKReBS"}}]}