{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYzWb1TJpWwViZCHpDRk16"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Before creating the linear regression model we need to import Pandas, Numpy and Tensorflow. We need Pandas for loading and manipulating the dataset, Numpy for numerical operations and TensorFlow for building and training the models."],"metadata":{"id":"IdKhsBAnAP8c"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"xTOCL243ANuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After that we need to load the data that will be used to train the model."],"metadata":{"id":"7o0RANwfAVf7"}},{"cell_type":"code","source":["df = pd.read_csv('https://raw.githubusercontent.com/23009256uhi/23009256_DataAnalytics/main/model_data.csv')"],"metadata":{"id":"Nou8joEXAXuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.head())\n","print(df.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FBxmINW_XS8","executionInfo":{"status":"ok","timestamp":1702473388294,"user_tz":0,"elapsed":5,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"67f522e2-dc9a-4933-eb6a-00abd7b0c56b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   day  temp  dewp  visib  NUM_COLLISIONS\n","0    2  59.8  50.2   10.0             475\n","1    6  38.6  34.4    9.6             806\n","2    4  43.5  30.4   10.0             510\n","3    5  77.1  62.0   10.0             565\n","4    6  78.2  69.6    9.3             581\n","              day        temp        dewp      visib  NUM_COLLISIONS\n","count  662.000000  662.000000  662.000000  662.00000      662.000000\n","mean     3.895770   59.225076   52.460423    8.54864      593.027190\n","std      1.987026   12.978881   14.408704    2.12104       89.757713\n","min      1.000000   22.200000    8.300000    1.50000      264.000000\n","25%      2.000000   51.000000   43.625000    7.70000      528.250000\n","50%      4.000000   63.000000   56.700000    9.70000      596.000000\n","75%      6.000000   68.875000   64.000000   10.00000      654.750000\n","max      7.000000   85.300000   73.600000   10.00000      841.000000\n"]}]},{"cell_type":"markdown","source":["Now we can calculate the standard deviation of 'NUM_COLLISIONS' and use it to normalise the target value. In other words, it will change the values to a common scale to improve the accuracy of the model."],"metadata":{"id":"qKYd05nsoVBY"}},{"cell_type":"code","source":["# Normalisation scale calculation. It calculates the standard deviation that will be used for normalising the target value.\n","SCALE_NUM_COLLISIONS = df['NUM_COLLISIONS'].std()"],"metadata":{"id":"zpur4zE4eRVy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now write a function to create and train a model which takes as arguments a given set of features."],"metadata":{"id":"LEDHgdswqF6n"}},{"cell_type":"code","source":["# Function to create and train a model for a given set of features\n","def train_model(features):\n","    # Select features from the df along with the target column \"NUM_COLLISIONS\" to create a new df\n","    df_input = df[features + [\"NUM_COLLISIONS\"]]\n","\n","    # Train-test split, training set (80% of the data) and test set (the remaining 20%). The training set is used to train the model, the test set is used to evaluate the model.\n","    training_set = df_input.sample(frac=0.8, random_state=0)\n","    test_set = df_input.drop(training_set.index)\n","\n","    # Separate features (input variables) and labels (target value)\n","    training_features = training_set.copy()\n","    test_features = test_set.copy()\n","\n","    # Extracting the target variable ('NUM_COLLISIONS') from the datasets and normalise the target values\n","    training_labels = training_features.pop('NUM_COLLISIONS') / SCALE_NUM_COLLISIONS\n","    test_labels = test_features.pop('NUM_COLLISIONS') / SCALE_NUM_COLLISIONS\n","\n","    # Feature normalisation using TensorFlow's Keras API. It takes the normalised features as input and outputs a single continuous value (the predicted \"NUM_COLLISIONS\").\n","    normaliser = tf.keras.layers.Normalization(axis=-1)\n","    normaliser.adapt(np.array(training_features))\n","\n","    # 1. Linear model definition using TensorFlow's Keras API. It takes the normalised features as input and outputs a single value (the predicted number of collisions).\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Input(shape=(training_features.shape[1],)),\n","        normaliser,\n","        layers.Dense(units=1)\n","    ])\n","\n","    # 2. Linear model compilation\n","    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error')\n","\n","    # 3. Linear model training, trained for 100 epochs, 20% of the training data used as validation set to monitor the model's performance during training. No training logs are printed during the process.\n","    history = model.fit(training_features, training_labels, epochs=100, verbose=0, validation_split=0.2)\n","\n","    # 4. Linear model evaluation using test_features and test_labels\n","    mae = model.evaluate(test_features, test_labels, verbose=0)\n","    print(f\"Mean Absolute Error for {features} model: {mae}\")\n","\n","    return model"],"metadata":{"id":"yywaEUDJkusY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can train a model now by using the function created before."],"metadata":{"id":"KOLc_XC_jUZd"}},{"cell_type":"code","source":["model = train_model(['day', 'dewp', 'temp', 'visib'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jsw_enrpwzLe","executionInfo":{"status":"ok","timestamp":1702473395558,"user_tz":0,"elapsed":7267,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"924b135c-6e92-4b9c-be84-f5ba578c612d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error for ['day', 'dewp', 'temp', 'visib'] model: 0.6733102202415466\n"]}]},{"cell_type":"markdown","source":["Let's test the model now with some made up test data. First we need to create an object with made up data."],"metadata":{"id":"wVdN87d7juDu"}},{"cell_type":"code","source":["# Made up data\n","input_custom = pd.DataFrame({\n","    'day': [1, 4, 7],\n","    'temp': [60, 70, 80],\n","    'dewp': [50, 60, 70],\n","    'visib': [10, 5, 8]\n","})"],"metadata":{"id":"9SYL4ZsEyrFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions_custom = model.predict(input_custom)\n","# Apply any necessary scaling factor if you used one during training\n","scaled_predictions = predictions_custom * SCALE_NUM_COLLISIONS\n","print(scaled_predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rn9bUB9kysvq","executionInfo":{"status":"ok","timestamp":1702473395559,"user_tz":0,"elapsed":11,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"b608b949-cf90-44f1-9909-a1dcb87c2692"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f54a0c65a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 53ms/step\n","[[597.0029 ]\n"," [661.0499 ]\n"," [746.83594]]\n"]}]},{"cell_type":"markdown","source":["It seems the model is working fine. Now we can try using the November collision data to check whether the accuracy of the model predictions. First we have to load the data and convert the 'collision_date' to datetime format.\n"],"metadata":{"id":"uoTFFqPigbhM"}},{"cell_type":"code","source":["df_nov = pd.read_csv('https://raw.githubusercontent.com/23009256uhi/23009256_DataAnalytics/main/november_collated_collision_data.csv')"],"metadata":{"id":"abTU0TZqgfqr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert 'collision_date' to datetime format\n","df_nov['collision_date'] = pd.to_datetime(df_nov['collision_date'])\n","\n","# Sort the DataFrame based on 'collision_date'\n","df_nov_sorted = df_nov.sort_values(by='collision_date')"],"metadata":{"id":"ums6gFJsq5TI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_nov_sorted.head())\n","print(df_nov_sorted.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLJGliZngyYV","executionInfo":{"status":"ok","timestamp":1702473395869,"user_tz":0,"elapsed":8,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"184ce545-b429-443b-90f5-a7b1894f0869"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    day  year  mo  da collision_date  temp  dewp     slp  visib  wdsp  mxpsd  \\\n","29    3  2023  11   1     2023-11-01  43.9  35.9  1014.6    9.5  11.2   17.1   \n","28    4  2023  11   2     2023-11-02  42.0  32.6  1027.2   10.0   7.7   11.1   \n","7     5  2023  11   3     2023-11-03  44.0  33.8  1032.0   10.0   8.2   15.0   \n","14    6  2023  11   4     2023-11-04  56.0  51.5  1024.6   10.0  11.2   18.1   \n","1     7  2023  11   5     2023-11-05  53.0  48.0  1016.7    9.4   5.0   11.1   \n","\n","     gust   max   min  prcp   sndp  fog  NUM_COLLISIONS  \n","29   27.0  48.9  39.9  0.06  999.9    0             257  \n","28   20.0  50.0  36.0  0.06  999.9    0             231  \n","7   999.9  53.1  30.9  0.00  999.9    0             290  \n","14   22.0  60.1  52.0  0.00  999.9    0             238  \n","1   999.9  61.0  43.0  0.00  999.9    0             235  \n","             day    year    mo         da       temp       dewp          slp  \\\n","count  30.000000    30.0  30.0  30.000000  30.000000  30.000000    30.000000   \n","mean    3.966667  2023.0  11.0  15.500000  45.440000  36.536667  1018.913333   \n","std     1.973677     0.0   0.0   8.803408   6.051708   9.468441     7.968374   \n","min     1.000000  2023.0  11.0   1.000000  34.100000  17.700000  1004.200000   \n","25%     2.250000  2023.0  11.0   8.250000  41.625000  29.600000  1014.450000   \n","50%     4.000000  2023.0  11.0  15.500000  45.200000  34.750000  1019.350000   \n","75%     5.750000  2023.0  11.0  22.750000  49.150000  44.675000  1025.800000   \n","max     7.000000  2023.0  11.0  30.000000  56.200000  52.700000  1033.000000   \n","\n","          visib       wdsp      mxpsd        gust        max        min  \\\n","count  30.00000  30.000000  30.000000   30.000000  30.000000  30.000000   \n","mean    9.58000  10.380000  17.133333  254.133333  54.093333  36.443333   \n","std     0.94701   3.672676   6.026455  418.515778   5.512489   6.569191   \n","min     6.20000   3.800000   8.000000   14.000000  43.000000  19.900000   \n","25%     9.55000   7.975000  13.000000   24.350000  50.000000  30.900000   \n","50%    10.00000  10.350000  17.100000   27.500000  54.000000  37.000000   \n","75%    10.00000  12.500000  19.525000   49.350000  59.550000  39.900000   \n","max    10.00000  21.800000  38.100000  999.900000  63.000000  52.000000   \n","\n","            prcp          sndp        fog  NUM_COLLISIONS  \n","count  30.000000  3.000000e+01  30.000000       30.000000  \n","mean    0.029000  9.999000e+02   0.066667      255.566667  \n","std     0.060819  2.312607e-13   0.253708       38.338495  \n","min     0.000000  9.999000e+02   0.000000      186.000000  \n","25%     0.000000  9.999000e+02   0.000000      231.500000  \n","50%     0.000000  9.999000e+02   0.000000      253.500000  \n","75%     0.010000  9.999000e+02   0.000000      275.000000  \n","max     0.260000  9.999000e+02   1.000000      352.000000  \n"]}]},{"cell_type":"markdown","source":["Now we can select the relevant features from the df_nov dataset that will be used to test the model."],"metadata":{"id":"hKDo-mMHFwZP"}},{"cell_type":"code","source":["# Select relevant features from df_nov\n","features_nov = df_nov_sorted[['day', 'dewp', 'temp', 'visib']]"],"metadata":{"id":"s996WlPkh-KC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(features_nov)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ORyQYVINkM0J","executionInfo":{"status":"ok","timestamp":1702473395869,"user_tz":0,"elapsed":6,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"791afd03-aa10-4191-97d7-b096782223d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    day  dewp  temp  visib\n","29    3  35.9  43.9    9.5\n","28    4  32.6  42.0   10.0\n","7     5  33.8  44.0   10.0\n","14    6  51.5  56.0   10.0\n","1     7  48.0  53.0    9.4\n","4     1  43.7  48.4    9.9\n","11    2  51.5  56.2    9.2\n","20    3  34.0  48.1   10.0\n","5     4  37.5  45.3    9.4\n","24    5  39.9  49.4   10.0\n","10    6  34.4  47.9   10.0\n","2     7  29.6  41.2   10.0\n","0     1  29.6  39.7   10.0\n","9     2  35.1  41.8   10.0\n","23    3  36.6  45.1   10.0\n","18    4  46.0  52.5    9.7\n","6     5  50.1  51.9    6.5\n","21    6  52.7  54.5    8.9\n","15    7  32.3  44.3   10.0\n","16    1  27.6  41.7   10.0\n","3     2  24.2  38.7   10.0\n","27    3  45.0  48.4    6.2\n","22    4  41.2  46.6   10.0\n","13    5  34.4  45.6   10.0\n","17    6  20.9  34.5   10.0\n","8     7  28.8  35.5   10.0\n","26    1  46.8  50.8    8.7\n","25    2  25.3  40.5   10.0\n","19    3  17.7  34.1   10.0\n","12    4  29.4  41.6   10.0\n"]}]},{"cell_type":"markdown","source":["We only need to call the predict method in TensorFlow's Keras API to generate predictions from the trained model and apply the scaling factor to the results."],"metadata":{"id":"VMcyG71tIx5S"}},{"cell_type":"code","source":["# Call the predict method\n","november_predictions = model.predict(features_nov)\n","\n","# Apply the scaling factor\n","scaled_nov_predictions = november_predictions * SCALE_NUM_COLLISIONS\n","\n","print(scaled_nov_predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCHTP7yfh-5R","executionInfo":{"status":"ok","timestamp":1702473396118,"user_tz":0,"elapsed":253,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"83d55bb5-94b0-41b4-b295-aedb20a405f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f54a0c65a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 67ms/step\n","[[547.33203]\n"," [562.7845 ]\n"," [583.49225]\n"," [645.4215 ]\n"," [661.2479 ]\n"," [523.3371 ]\n"," [551.5275 ]\n"," [522.8242 ]\n"," [572.092  ]\n"," [592.0107 ]\n"," [593.7676 ]\n"," [619.4694 ]\n"," [488.99313]\n"," [530.3336 ]\n"," [547.378  ]\n"," [586.05817]\n"," [622.5125 ]\n"," [654.2202 ]\n"," [620.4385 ]\n"," [471.31992]\n"," [489.2658 ]\n"," [565.1053 ]\n"," [586.6853 ]\n"," [580.11194]\n"," [580.6345 ]\n"," [638.0524 ]\n"," [525.7639 ]\n"," [487.54123]\n"," [498.36685]\n"," [548.7148 ]]\n"]}]},{"cell_type":"markdown","source":["Let's now create a new data frame and add the predicted collisions and the actual collisions to check the accuracy of the model."],"metadata":{"id":"MPxLXK_NNLOg"}},{"cell_type":"code","source":["# Convert the predictions to a DataFrame\n","predictions_df = pd.DataFrame(scaled_nov_predictions, columns=['Predicted_Collisions'])\n","\n","df_nov_sorted['Predicted_Collisions'] = predictions_df['Predicted_Collisions'].values"],"metadata":{"id":"DvGqieOfr_0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_nov_sorted[['collision_date', 'NUM_COLLISIONS', 'Predicted_Collisions']])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zVXvuib3sP2n","executionInfo":{"status":"ok","timestamp":1702473396119,"user_tz":0,"elapsed":6,"user":{"displayName":"Simone Cennami","userId":"11926110532086132887"}},"outputId":"16a31d1b-fd8d-4327-ddc0-7d6882111526"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   collision_date  NUM_COLLISIONS  Predicted_Collisions\n","29     2023-11-01             257            547.332031\n","28     2023-11-02             231            562.784485\n","7      2023-11-03             290            583.492249\n","14     2023-11-04             238            645.421509\n","1      2023-11-05             235            661.247925\n","4      2023-11-06             275            523.337097\n","11     2023-11-07             233            551.527527\n","20     2023-11-08             267            522.824219\n","5      2023-11-09             240            572.091980\n","24     2023-11-10             215            592.010681\n","10     2023-11-11             294            593.767578\n","2      2023-11-12             233            619.469421\n","0      2023-11-13             300            488.993134\n","9      2023-11-14             263            530.333618\n","23     2023-11-15             271            547.377991\n","18     2023-11-16             270            586.058167\n","6      2023-11-17             320            622.512512\n","21     2023-11-18             275            654.220215\n","15     2023-11-19             266            620.438477\n","16     2023-11-20             315            471.319916\n","3      2023-11-21             352            489.265808\n","27     2023-11-22             285            565.105286\n","22     2023-11-23             186            586.685303\n","13     2023-11-24             216            580.111938\n","17     2023-11-25             208            580.634521\n","8      2023-11-26             203            638.052429\n","26     2023-11-27             250            525.763916\n","25     2023-11-28             217            487.541229\n","19     2023-11-29             220            498.366852\n","12     2023-11-30             242            548.714783\n"]}]},{"cell_type":"markdown","source":["It looks like the model's predictions are about twice the actual number of collisions. This discrepancy between the actual and predicted number of collisions could be due to various factors. One possible reason might be the impact of COVID-19. In fact, the pandemic has led to changes in traffic volumes and travel patterns, as many people have started working remotely, along with behavioral changes. The next step will be to create a DNN model to better understand and predict these patterns."],"metadata":{"id":"XO3GP1SGObCP"}}]}